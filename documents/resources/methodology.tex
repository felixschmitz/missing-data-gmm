\section{Methodology}
Assume a standard linear regression model of the form
\begin{equation}\label{eq:1}
    y_i = \alpha_0 x_i + z_i^{\prime} \beta_0 + \epsilon_i = w_i^{\prime} \lambda_0 + \epsilon_i, \quad i= 1,\ldots,n
\end{equation}
where $x_i$ is a possibly missing scalar regressor and $z_i$ contains an intercept ($z_{1i}$) and further instrument(s) ($z_{2i}, \ldots$).
Further, we have standard exogeneity assumptions on the residual $\epsilon_i$, that is $\mathbb{E}[x_i \epsilon_i] = 0$ and $\mathbb{E}[z_i \epsilon_i] = 0$.
For the sake of simplicity, we assume that $z_i$ has only two columns, i.e. $z_i = (1, z_{2i})$, the intercept and a single fully-observed and scalar instrument, the results are generalizable nevertheless.
Let $w_i \equiv (x_i, z_i)^{\prime}$, and further $\lambda_0 \equiv (\alpha_0, \beta_0^{\prime})^{\prime}$.
The missingness of $x_i$ is denoted by $$m_i = \mathbbm{1}\{x_i \text{ missing}\}.$$
The required assumptions on the missingness structure can be subsumed to lie between the cases of missing at random (MAR) and missing completely at random (MCAR), where the former is a weaker assumption than the latter.

Further, assume a linear projection of $x_i$ onto $z_i$ of the form
\begin{equation}\label{eq:2}
    x_i = z_i^{\prime} \gamma_0 + \xi_i
\end{equation}
where $\mathbb{E}[z_i \xi_i] = 0$, that is $z_i$ is uncorrelated with the residual $\xi_i$ of this imputation model.
Plugging Equation \eqref{eq:2} into \eqref{eq:1} yields the following model
\begin{equation}\label{eq:3}
    y_i = z_i^{\prime} (\gamma_0 \alpha_0 + \beta_0) + \epsilon_i + \xi_i \alpha_0 .
\end{equation}

The relevant missingness assumptions are $$\text{a) }\mathbb{E}[m_i z_i \epsilon_i] = 0, \text{b) }\mathbb{E}[m_i z_i \xi_i] = 0, \text{c) }\mathbb{E}[m_i x_i \epsilon_i] = 0.$$
These are central to the identification of the model parameters and the consistency of the GMM estimator.
Assumption a) is the standard exogeneity assumption of the residual in the regression model from Equation \eqref{eq:1}.
It describes the absence of correlation between the missingness indicator $m_i$ and the residual $\epsilon_i$ conditional on the instrument $z_i$ and is required for the consistency of the OLS estimator of $\alpha_0$.
Assumption b) is on the exogeneity of the imputation residual $\xi_i$.
It states that the missingness of $x_i$, $m_i$, is uncorrelated with the imputation residual $\xi_i$ when conditioning on $x_i$.
Facilitating consistent estimation of $\gamma_0$ in the imputation model \eqref{eq:2}.
Assumption c) is on the exogeneity of the potentially missing regressor $z_i$.
It states that the missingness of $x_i$, $m_i$, is uncorrelated with the residual $\epsilon_i$ in the regression model from Equation \eqref{eq:1} when controlled for $x_i$.
This assumption is required for the consistency of the estimator of $\beta_0$.
Conceptually, the missingness assumptions a)-c) state that both the regression model in Equation \eqref{eq:1} and the imputation model in Equation \eqref{eq:3} are equivalent for missing and non-missing data.
Hence, under these assumptions about the missing data structure, the model parameters $\alpha_0$ and $\beta_0$ in Equation \eqref{eq:3} can under the correctly specified imputation model be consistently estimated using OLS.

Recapitulate that the assumptions on the missingness structure are less restrictive than the MCAR assumption, but stronger than the MAR assumption.
Under MCAR, the missingness of $x_i$, $m_i$, is statistically independent of all observed variables ($x_i, z_i$) and unobserved residuals ($\epsilon_i, \xi_i$).
This assumption entails the three missingness assumptions a), b), and c) and is thus stronger than the missing data assumptions required for the identification of the model parameters and consistency of the GMM estimator.
In contrast to MCAR, the MAR assumption allows the missingness of $x_i$, $m_i$, to depend on the observed variable $z_i$ and the dependent variable $y_i$, but not on the partially missing variable $x_i$.
This assumption is less restrictive than the missingness assumptions required for the identification of the model parameters and the consistency of the GMM estimator (observe part c)).
For the complete data method, assumptions a) and c) are sufficient for consistency.
Highlighting the efficiency gains possible for the GMM estimator over the complete data method through the missing data assumptions.

\subsection{Generalized Method of Moments}
The GMM is an estimation technique relying on moment conditions derived from the model assumptions, rather than requiring a fully specified likelihood function.
The moment conditions are expectations of functions of the observed data and the model parameters.
These capture the theoretical relationships implied by the model without fully specifying the underlying data distribution.
The method is particularly useful in econometric models with endogeneity or missing data, as it leverages instrumental variables to identify and consistently estimate model parameters.
In the context of the regression model specified above through equations \eqref{eq:1}, \eqref{eq:2}, and \eqref{eq:3}, the moment conditions are described by the following functions:
\begin{equation}\label{eq:4}
    g_i(\alpha, \beta, \gamma) = \begin{pmatrix}
        (1-m_i)w_i(y_i - \alpha x_i - z_i^{\prime}\beta) \\
        (1-m_i)z_i (x_i - z_i^{\prime} \gamma) \\
        m_i z_i (y_i - z_i^{\prime} (\gamma \alpha + \beta))
    \end{pmatrix}
    = \begin{pmatrix}
        g_{1i}(\alpha, \beta, \gamma) \\
        g_{2i}(\alpha, \beta, \gamma) \\
        g_{3i}(\alpha, \beta, \gamma)
    \end{pmatrix}
\end{equation}
and the assumptions on the missing data structure imply that $\mathbb{E}[g_i(\alpha_0, \beta_0, \gamma_0)] = 0$.
Hence, the number of moment conditions is larger than the dimension of the vector of parameters $(\alpha_0, \beta_0, \gamma_0)$ to estimate, which makes the model overidentified.
The first moment condition $g_{1i}$ estimates $\lambda_0$ for observed $x_i$ based on Equation \eqref{eq:1}.
The second moment condition $g_{2i}$ stems from the imputation model in Equation \eqref{eq:2}, which has to hold for all $x_i$, observed and missing.
The third moment condition $g_{3i}$ is the product of the missingness indicator $m_i$ and the imputation model in Equation \eqref{eq:3}, it describes the adequate regression equation for missing values.
An adapted GMM estimator omitting $g_{2i}$ or $g_{3i}$, is equivalent to the complete data method estimator.

The GMM estimator is then obtained by minimizing the sample analog of the quadratic form of the moment conditions, i.e. the sample analog of the expectation of the moment conditions.
It is consistent and asymptotically normally distributed under standard regularity conditions and is also robust to misspecification of the imputation model, as long as the moment conditions are correctly specified.
Further, an overidentification test can be used to test the validity of the moment conditions and the model assumptions.

\subsection{The Dummy Variable Method}
\label{subsec:methodology-dummy}
The dummy variable method is a simple and widely used approach to handle missing data.
The assumptions on the missing data structure do not guarantee consistency of the corresponding estimator, but neither does a stricter assumption such as MCAR.
The method is based on the idea of creating a dummy variable for the missingness of the regressor, and filling missing values of $x_i$ with a constant.
The dummy variable is then included in the regression model as an additional regressor.

Combining Equations \eqref{eq:1} and \eqref{eq:3} for missing and observed data, we obtain:

\begin{equation}\label{eq:5}
    y_i = (1-m_i) x_i \alpha_0 + z_i^{\prime} \beta_0 + m_i z_i^{\prime} \gamma_0 \alpha_0 + \epsilon_i + m_i \xi_i \alpha_0.
\end{equation}

This model contains the information of both the true regression model for observed data $(m_i = 0)$ and the imputation model for missing data $(m_i = 1)$.
That is, $(1-m_i) x_i \alpha_0$ is the contribution of the true regression model for observed data.
$m_i z_i^{\prime} \gamma_0 \alpha_0$ is the contribution of the imputation model for missing data.
$\epsilon_i + m_i \xi_i \alpha_0$ is the combined residual of the true regression and imputation models.
The instruments $z_i^{\prime} \beta_0$ are included in the model, as well.
It is a valid regression model under the assumption that the missingness of $x_i$ is conditionally independent of the combined residual $\epsilon_i + m_i \xi_i \alpha_0$.

Estimating the model in Equation \eqref{eq:5} by OLS yields consistent estimates of the parameters $(\alpha_0, \beta_0)$.
These are equivalent to the complete data method estimates.
The efficiency gains from the GMM estimator over the complete data method in this model come from the correct specification of the imputation model also holding on the observed data.
Rewriting Equation \eqref{eq:5} with $z_i = (1, z_{2i}^{\prime})$ and $\gamma_0 = (\gamma_{10}, \gamma_{20}^{\prime})$ yields:

\begin{equation}\label{eq:6}
    y_i = (1-m_i) x_i \alpha_0 + z_i^{\prime} \beta_0 + m_i \gamma_{10} \alpha_0 + m_i z_{2i}^{\prime} \gamma_{20} \alpha_0 + \epsilon_i + m_i \xi_i \alpha_0.
\end{equation}

Since, the dummy variable method does not impose assumptions (especially not assumptions a) - c) on the missing data structure), the method performs OLS on the following model:

\begin{equation}\label{eq:7}
    y_i = (1-m_i) x_i \alpha_0 + z_i^{\prime} \beta_0 + m_i \gamma_{10} \alpha_0 + \epsilon_i + m_i \xi_i \alpha_0
\end{equation}

where the regressor $m_i z_{2i}^{\prime}$ is omitted, producing an omitted-variable bias.
\citet{jones1996} shows that the OLS estimators of the parameters $(\alpha_0, \beta_0)$ from the regression model in Equation \eqref{eq:7} are biased and inconsistent, unless either of (a) $\alpha_0 = 0$ or (b) $\gamma_{20} = 0$ holds.
For condition (a), the regressor $x_i$ is irrelevant in the regression model from Equation \eqref{eq:1}.
Hence, $x_i$ does not contribute to explaining the variation in $y_i$ and should simply be dropped from the model.
Since, condition (a) is straightforward to test and the solution easy to apply, it is an intriguing solution to the omitted-variable bias of the dummy variable method.
If the regressor $x_i$ is irrelevant and excluded from the model, there is no need to worry about a method being sensitive to the missing data mechanism, after all.

For condition (b), the instrument $z_{2i}$ is irrelevant in the imputation model from Equation \eqref{eq:2}.
Hence, $z_{2i}$ does not contribute to explaining the variation in $x_i$, and it becomes irrelevant to include in the imputation model.
Yet, assuming that the instrument $z_{2i}$ is unable to explain any variation in $x_i$ is a strong assumption, and highly unlikely in practice.
The following Monte Carlo simulations thus focus on showing the bias and inconsistency of the dummy variable method when the residuals are becoming more complex, i.e. when introducing heteroskedasticity, and when the instrument $z_{2i}$ is becoming more relevant in the imputation model through values of $\gamma_{20}$ different from $0$.
